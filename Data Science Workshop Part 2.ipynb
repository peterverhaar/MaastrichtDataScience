{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} nltk\n",
    "!conda install --yes --prefix {sys.prefix} pandas\n",
    "!conda install --yes --prefix {sys.prefix} seaborn\n",
    "!conda install --yes --prefix {sys.prefix} gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nltk\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install seaborn\n",
    "!{sys.executable} -m pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# function to tokenise a string into words\n",
    "def tokenise( text ):\n",
    "    tokens = []\n",
    "    text = text.lower()\n",
    "    text = re.sub( '--' , ' -- ' , text)\n",
    "    words = re.split( r'\\s+' , text )\n",
    "    for w in words:\n",
    "        w = w.strip( string.punctuation )\n",
    "        if re.search( r\"[a-zA-Z']\" , w ):\n",
    "            tokens.append(w)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "freq = dict()\n",
    "\n",
    "\n",
    "file = '766-0.txt'\n",
    "\n",
    "try:\n",
    "    text = open( file , encoding = 'utf-8' )\n",
    "except:\n",
    "    print( \"Cannot read \" + file + \" !\" )\n",
    "\n",
    "\n",
    "for line in text:\n",
    "    words = tokenise( line )\n",
    "    for w in words:\n",
    "        freq[w] = freq.get( w , 0 ) + 1\n",
    "\n",
    "sortedList = reversed( sorted( freq , key=lambda x: freq[x]) )\n",
    "\n",
    "for w in sortedList:\n",
    "    print( w + ' => ' + str( freq[w] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# A frequency list filtered using a list of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "\n",
    "url = \"http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words\"\n",
    "\n",
    "request = urllib.request.urlopen(url)\n",
    "bytes = request.read()\n",
    "contents = bytes.decode(\"utf-8\")\n",
    "request.close()\n",
    "\n",
    "contents = re.sub( r'\\s+' , ' ' , contents )\n",
    "stopwords = re.split( ' ' , contents )\n",
    "\n",
    "freq = dict()\n",
    "\n",
    "\n",
    "file = '766-0.txt'\n",
    "\n",
    "try:\n",
    "    text = open( file , encoding = 'utf-8' )\n",
    "except:\n",
    "    print( \"Cannot read \" + file + \" !\" )\n",
    "\n",
    "\n",
    "for line in text:\n",
    "    words = tokenise( line )\n",
    "    for w in words:\n",
    "        if w not in stopwords:\n",
    "            freq[w] = freq.get( w , 0 ) + 1\n",
    "\n",
    "\n",
    "sortedList = reversed( sorted( freq , key=lambda x: freq[x]) )\n",
    "for w in sortedList:\n",
    "    print( w + ' => ' + str( freq[w] ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# A word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from wordcloud import WordCloud \n",
    "\n",
    "wordcloud = WordCloud( background_color=\"white\",  width=4500,height=4500, max_words= 100,relative_scaling=1,normalize_plurals=False).generate_from_frequencies(freq)\n",
    "\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchTerm = 'fire'\n",
    "\n",
    "file = '766-0.txt'\n",
    "\n",
    "try:\n",
    "    text = open( file , encoding = 'utf-8' )\n",
    "except:\n",
    "    print( \"Cannot read \" + file + \" !\" )\n",
    "    \n",
    "    \n",
    "concordance = []\n",
    "\n",
    "window = 30\n",
    "\n",
    "for line in text:\n",
    "    line = line.strip()\n",
    "    if re.search( regex , line , re.IGNORECASE ):\n",
    "\n",
    "        extract = ''\n",
    "\n",
    "        position = re.search( regex , line , re.IGNORECASE ).start()\n",
    "        start = position - len( searchTerm ) - window ;\n",
    "        fragmentLength = start + 2 * window  + len( searchTerm )\n",
    "        if fragmentLength > len( line ):\n",
    "            fragmentLength = len( line )\n",
    "\n",
    "        if start < 0:\n",
    "            whiteSpace = ''\n",
    "            i = 0\n",
    "            while i < abs(start):\n",
    "                whiteSpace += ' '\n",
    "                i += 1\n",
    "            extract = whiteSpace + line[ 0 : fragmentLength ]\n",
    "        else:\n",
    "            extract = line[ start : fragmentLength ]\n",
    "\n",
    "        if re.search( '\\w' , extract ) and re.search( regex , extract ):\n",
    "            concordance.append( extract )\n",
    "\n",
    "for c in concordance:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Module\n",
    "\n",
    "The code below demonstrates the use of the methods sent_tokenize() from the nltk module. The code calculates data about the number of sentences in the opening paragraph from Hemingways novel *A Farewell to Arms*. Try to change the code in such a way that it can be used to produce counts of the number of sentences and the number of words in your own corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "quote = '''\n",
    "In the late summer of that year we lived in a house in a village that looked across the river and the plain to the mountains. In the bed of the river there were pebbles and boulders, dry and white in the sun, and the water was clear and swiftly moving and blue in the channels. Troops went by the house and down the road and the dust they raised powdered the leaves of the trees. The trunks of the trees too were dusty and the leaves fell early that year and we saw the troops marching along the road and the dust rising and leaves, stirred by the breeze, falling and the soldiers marching and afterward the road bare and white except for the leaves.\n",
    "'''\n",
    "\n",
    "\n",
    "sentences = sent_tokenize(quote)\n",
    "\n",
    "print( len(sentences) )\n",
    "\n",
    "for s in sentences:\n",
    "    print(s)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging\n",
    "\n",
    "The code below demonstrates the use of the function pos_tag(), which can be used to generate part of speech tags. Using the code below as a basis, try to produce counts of the number of nouns in the text (either singular or plural). The codes for nouns are 'NN', 'NNP', 'NNS' and 'NNPS'. See also https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "quote = '''\n",
    "The studio was filled with the rich odour of roses, and when the light summer wind stirred amidst the trees of the garden, there came through the open door the heavy scent of the lilac, or the more delicate perfume of the pink-flowering thorn.\n",
    "'''\n",
    "\n",
    "words = word_tokenize(quote)\n",
    "pos = nltk.pos_tag(words)\n",
    "\n",
    "for p in pos:\n",
    "    print(p[0] + ' => ' + p[1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatisation\n",
    "\n",
    "Lemmatisation is a process in which the conjugated forms of the words that are found in a text are converted to their base dictionary form. This base form is referred to as the lemma. Using the code below as a basis, try to produce a lemmatised version of one of the texts in your own corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "quote = \"It was the best of times, it was the worst of times\"\n",
    "\n",
    "words = words = word_tokenize(quote)\n",
    "(quote)\n",
    "pos = nltk.pos_tag(words)\n",
    "\n",
    "for i in range( 0 , len(words) ):\n",
    "    posTag = ''\n",
    "    if re.search( '^v' ,  pos[i][1] , re.IGNORECASE ):\n",
    "        posTag = 'v'\n",
    "    elif re.search( '^j', pos[i][1] , re.IGNORECASE ):\n",
    "        posTag = 'a'\n",
    "        \n",
    "\n",
    "    if posTag == 'v' or posTag == 'v':\n",
    "        print( words[i] + ' => ' + \n",
    "           lemmatiser.lemmatize( words[i] , posTag ) )\n",
    "    else:\n",
    "        print( words[i] + ' => ' + \n",
    "            lemmatiser.lemmatize( words[i] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular expressions\n",
    "\n",
    "Can you find all the hashtags and all the user names in the tweets that were downloaded during part 1 of the workshop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "tweets = open('tweets.txt')\n",
    "\n",
    "twitterUsers = dict()\n",
    "tags = dict()\n",
    "\n",
    "for t in tweets:\n",
    "    ## Enter regular expression as first parameter of findall()\n",
    "    users = re.findall( r'' , t)\n",
    "    for u in users:\n",
    "        twitterUsers[u] = twitterUsers.get( u , 0 ) + 1\n",
    "\n",
    "    hashTags = re.findall( r'' , t)\n",
    "    for h in hashTags:\n",
    "        tags[h] = tags.get( h , 0 ) + 1\n",
    "\n",
    "for u in twitterUsers:\n",
    "    print(u)\n",
    "\n",
    "for ht in tags:\n",
    "    print(ht)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular expressions in Ulysses\n",
    "\n",
    "Download the novel *Ulysses* from Project Gutenberg, using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = \"http://www.gutenberg.org/files/4300/4300-0.txt\"\n",
    "\n",
    "import urllib.request\n",
    "import re\n",
    "import time\n",
    "\n",
    "def download( url ):\n",
    "\n",
    "    request = urllib.request.urlopen(url)\n",
    "    bytes = request.read()\n",
    "    fullText = bytes.decode(\"utf-8\")\n",
    "    request.close()\n",
    "\n",
    "    parts = re.split( '/' , url )\n",
    "    id = parts[-1]\n",
    "\n",
    "    out = open( id , 'w' , encoding = 'utf-8')\n",
    "    out.write( fullText )\n",
    "    out.close()\n",
    "    time.sleep( 3 )\n",
    "\n",
    "\n",
    "download('http://www.gutenberg.org/files/4300/4300-0.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this novel, find all the text fragments containing a year (e.g. the sentence “What reflection concerning the irregular sequence of dates 1884, 1885, 1886, 1888, 1892, 1893, 1904 did Bloom make before their arrival at their destination?”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "novel = open(\"4300-0.txt\" , encoding='utf-8')\n",
    "lines = []\n",
    "\n",
    "\n",
    "for line in novel:\n",
    "    lines.append(line)\n",
    "\n",
    "\n",
    "\n",
    "for line in lines:\n",
    "    if re.search( r'\\d{4}' , line ):\n",
    "        print( line )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find fragments in which Joyce chose the dramatic form, or, more specifically, lines which begin with the name of a speaker in capitals, followed directly by a colon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in lines:\n",
    "    if re.search( r'[A-Z]+:' , line ):\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all the lines which contain either the singular or the plural form of \"star\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in lines:\n",
    "    if re.search( r'\\bstars?\\b' , line ):\n",
    "        print( line )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find passages which contain at least two words that begin with \"br\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in lines:\n",
    "    if re.search( r'\\bbr.+\\bbr.*' , line ):\n",
    "        print( line )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas\n",
    "\n",
    "Using the pandas library, open the csv file named 'vanGogh.csv' and print the following:\n",
    "\n",
    "* information about the number of rows and the number of columns\n",
    "* print the first 3 rows\n",
    "* print a list of all column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv( 'vanGogh.csv' )\n",
    "\n",
    "print( df.shape )\n",
    "print( df.head(2) )\n",
    "print( df.columns )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the pandas library, calculate the correlations between the columns in the csv file. Additionally, print a list of the means of all the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the module 'seaborn'. Use the seaborn libraries and the pandas libraries to create a heatmap that visualises the correlations between all the columns in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv( 'vanGogh.csv' )\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "ax = sns.heatmap( df.corr() , linewidth=0.5 , cmap=\"YlGnBu\" )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matplotlib\n",
    "\n",
    "A scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv( 'vanGogh.csv' )\n",
    "    \n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "fig = plt.figure( )\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.scatter( x.values() , y.values()  , alpha=0.8, edgecolors='none', s=30, label=None )\n",
    "ax.set_xlabel('Type-token ratio')\n",
    "ax.set_ylabel('Average nr of words')\n",
    "\n",
    "ax.set_title( 'Language of Vincent van Gogh')\n",
    "\n",
    "for label in x.keys():\n",
    "    ax.annotate( label , (x[label] , y[label] + 0.4))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A line chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv( 'vanGogh.csv' )\n",
    "    \n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "fig = plt.figure( )\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.scatter( x.values() , y.values()  , alpha=0.8, edgecolors='none', s=30, label=None )\n",
    "ax.set_xlabel('Type-token ratio')\n",
    "ax.set_ylabel('Average nr of words')\n",
    "\n",
    "ax.set_title( 'Language of Vincent van Gogh')\n",
    "\n",
    "for label in x.keys():\n",
    "    ax.annotate( label , (x[label] , y[label] + 0.4))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dispersion graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### code to divide the novel into segments.\n",
    "### The number of segments is determined by variable numberOfSegments\n",
    "\n",
    "import re\n",
    "\n",
    "numberOfSegments = 30\n",
    "segments = []\n",
    "\n",
    "novel = open('APortraitOfTheArtist.txt')\n",
    "\n",
    "## The read() function can read in the entire file as a single string\n",
    "fullText = novel.read()\n",
    "allWords = re.split( r'\\s+' , fullText )\n",
    "\n",
    "segmentSize = int( len(allWords) / numberOfSegments )\n",
    "\n",
    "countWords = 0 \n",
    "text = ''\n",
    "\n",
    "for word in allWords:\n",
    "    countWords += 1\n",
    "    text += word + ' ' \n",
    "    \n",
    "    ## This line below used the modulo operator:\n",
    "    ## We can use it to test if the first number is \n",
    "    ## divisible by the second number\n",
    "    if countWords % segmentSize == 0:\n",
    "        segments.append(text.strip())\n",
    "        text = ''\n",
    "\n",
    "        \n",
    "data = dict()\n",
    "        \n",
    "count = 0 \n",
    "for s in segments:\n",
    "    count += 1\n",
    "    hits = re.findall( r'\\bart(ist)?' , s , re.IGNORECASE )\n",
    "    data[ count ] = len( hits )\n",
    "    \n",
    "\n",
    "## This next line is needed to visualise the data within the Notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "fig = plt.figure( figsize=( 12 , 4 ) )\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.plot( data.keys() , data.values() , color = '#930d08' , linestyle = 'solid')\n",
    "\n",
    "ax.set_xlabel('Section')\n",
    "ax.set_ylabel('Frequency')\n",
    "\n",
    "ax.set_title( 'A Portrait of the Artist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "textFile = 'APortraitOfTheArtist.txt'\n",
    "maxNrWords = 50\n",
    "\n",
    "## function to tokenise a string into words\n",
    "def tokenise( text ):\n",
    "    tokens = []\n",
    "    text = text.lower()\n",
    "    text = re.sub( '--' , ' -- ' , text)\n",
    "    words = re.split( r'\\s+' , text )\n",
    "    for w in words:\n",
    "        w = w.strip( string.punctuation )\n",
    "        if re.search( r\"[a-zA-Z']\" , w ):\n",
    "            tokens.append(w)\n",
    "    return tokens\n",
    "\n",
    "novel = open( textFile )\n",
    "\n",
    "## Calculate the frequencies of all the words\n",
    "freq = dict()\n",
    "\n",
    "for paragraph in novel:\n",
    "    words = tokenise(paragraph)\n",
    "    for w in words:\n",
    "        freq[w] = freq.get(w,0)+1\n",
    "            \n",
    "\n",
    "## determine the 50 most frequent words, and \n",
    "## place these in a dictionary named mostFreq()\n",
    "\n",
    "sortedWords = reversed( sorted( freq , key=lambda x: freq[x]) )\n",
    "mostFreq = dict()\n",
    "\n",
    "count = 0 \n",
    "for w in sortedWords:\n",
    "    mostFreq[w] = freq[w]\n",
    "    count += 1\n",
    "    if count == maxNrWords:\n",
    "        break\n",
    "        \n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure( figsize=( 12 , 5 ) )\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.bar( mostFreq.keys() , mostFreq.values() , width = 0.9 , alpha = 0.5 , color = '#03017a')\n",
    "\n",
    "ax.set_xlabel('Words')\n",
    "ax.set_ylabel('Frequencies')\n",
    "ax.set_title( 'A Portrait of the Artist as a Young Man')\n",
    "\n",
    "## labels for the ticks on the X-axis need to \n",
    "## be shown vertically to improve the readability\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "word2vec_model_file = \"egodocumenten.txt\"\n",
    "ed_model = gensim.models.KeyedVectors.load( word2vec_model_file )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in ('java','soldaat','japanner','molukken'):\n",
    "    if word in ed_model:\n",
    "        print(word,ed_model.most_similar(word))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
